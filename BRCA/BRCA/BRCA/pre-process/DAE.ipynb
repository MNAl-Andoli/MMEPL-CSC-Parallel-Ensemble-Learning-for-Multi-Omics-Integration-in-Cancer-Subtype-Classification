{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd50c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 10:20:37.266571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 10:20:37.404597: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 10:20:37.404634: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 10:20:38.079718: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 10:20:38.079810: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 10:20:38.079820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 224)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the CSV files as numpy arrays\n",
    "data1 = np.genfromtxt('Data/gistic_data.csv', delimiter=',')# shape (num_samples, input_dim1)\n",
    "data1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150d759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the column name andsample names\n",
    "data1=data1[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a6ff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 10:20:39.168343: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-08 10:20:39.168380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (alandoli-Lenovo-Z50-70): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 10:20:39.168661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 374ms/step - loss: 0.6169 - val_loss: 0.5650\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.5191 - val_loss: 0.5611\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.4615 - val_loss: 0.5597\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.4173 - val_loss: 0.5602\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3824 - val_loss: 0.5619\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.3568 - val_loss: 0.5605\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.3357 - val_loss: 0.5526\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.3190 - val_loss: 0.5371\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.3051 - val_loss: 0.5163\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.2953 - val_loss: 0.4954\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.2873 - val_loss: 0.4733\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2792 - val_loss: 0.4503\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2723 - val_loss: 0.4309\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.2669 - val_loss: 0.4165\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.2623 - val_loss: 0.4011\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.2574 - val_loss: 0.3860\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2540 - val_loss: 0.3741\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2529 - val_loss: 0.3636\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.2490 - val_loss: 0.3529\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.2466 - val_loss: 0.3444\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2454 - val_loss: 0.3370\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2433 - val_loss: 0.3303\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.2417 - val_loss: 0.3251\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2413 - val_loss: 0.3208\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2387 - val_loss: 0.3154\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2371 - val_loss: 0.3103\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2367 - val_loss: 0.3071\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.2339 - val_loss: 0.3039\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2387 - val_loss: 0.2984\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2332 - val_loss: 0.2963\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2335 - val_loss: 0.2952\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.2319 - val_loss: 0.2930\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2298 - val_loss: 0.2909\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2302 - val_loss: 0.2888\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.2281 - val_loss: 0.2868\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.2284 - val_loss: 0.2849\n",
      "Epoch 37/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2194"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_shape = (data1.shape[1],)\n",
    "\n",
    "# Define the model architecture\n",
    "encoder = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2048, activation=\"relu\", input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "decoder = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(256, activation=\"relu\", input_shape=(128,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(2048, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(data1.shape[1], activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder model\n",
    "autoencoder = keras.Sequential([encoder, decoder])\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(data1, data1, epochs=100, batch_size=256, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model\n",
    "encoded_data = encoder.predict(data1)\n",
    "np.savetxt('latent_gistic.csv', encoded_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e346ec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6241, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb264be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 224)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the clear_sorted_new_L4_match_miRNA.csv  CSV files as numpy arrays\n",
    "data2 = np.genfromtxt('Data/rppa_data.csv', delimiter=',')# shape (num_samples, input_dim1)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b867472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the column name andsample names\n",
    "data2=data2[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404bd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 151ms/step - loss: 0.5962 - val_loss: 0.5660\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5805 - val_loss: 0.5633\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5676 - val_loss: 0.5603\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5559 - val_loss: 0.5572\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5452 - val_loss: 0.5538\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5354 - val_loss: 0.5502\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5266 - val_loss: 0.5465\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5185 - val_loss: 0.5428\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5106 - val_loss: 0.5393\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5038 - val_loss: 0.5360\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4975 - val_loss: 0.5330\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4918 - val_loss: 0.5305\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4863 - val_loss: 0.5282\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4813 - val_loss: 0.5263\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4765 - val_loss: 0.5245\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4720 - val_loss: 0.5230\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4675 - val_loss: 0.5216\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4626 - val_loss: 0.5202\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4585 - val_loss: 0.5187\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4537 - val_loss: 0.5172\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4491 - val_loss: 0.5154\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4448 - val_loss: 0.5134\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4401 - val_loss: 0.5112\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4356 - val_loss: 0.5085\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4309 - val_loss: 0.5054\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4266 - val_loss: 0.5018\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4211 - val_loss: 0.4976\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4164 - val_loss: 0.4930\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4114 - val_loss: 0.4880\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4067 - val_loss: 0.4828\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4018 - val_loss: 0.4774\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3967 - val_loss: 0.4716\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3914 - val_loss: 0.4651\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3865 - val_loss: 0.4584\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3811 - val_loss: 0.4515\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3763 - val_loss: 0.4444\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3713 - val_loss: 0.4369\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3662 - val_loss: 0.4292\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3613 - val_loss: 0.4216\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3563 - val_loss: 0.4143\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3516 - val_loss: 0.4074\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3462 - val_loss: 0.4001\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3420 - val_loss: 0.3930\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3372 - val_loss: 0.3863\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3327 - val_loss: 0.3801\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3274 - val_loss: 0.3738\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3238 - val_loss: 0.3675\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3193 - val_loss: 0.3617\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3154 - val_loss: 0.3562\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3124 - val_loss: 0.3508\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3079 - val_loss: 0.3458\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3040 - val_loss: 0.3408\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3007 - val_loss: 0.3362\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2990 - val_loss: 0.3323\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2946 - val_loss: 0.3285\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2916 - val_loss: 0.3248\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2899 - val_loss: 0.3212\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2860 - val_loss: 0.3176\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2835 - val_loss: 0.3137\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2813 - val_loss: 0.3101\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2790 - val_loss: 0.3069\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2768 - val_loss: 0.3046\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2746 - val_loss: 0.3021\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2723 - val_loss: 0.2996\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2709 - val_loss: 0.2966\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2693 - val_loss: 0.2941\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2678 - val_loss: 0.2918\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2654 - val_loss: 0.2901\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2666 - val_loss: 0.2879\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2632 - val_loss: 0.2859\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2617 - val_loss: 0.2835\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2605 - val_loss: 0.2815\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2598 - val_loss: 0.2798\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2575 - val_loss: 0.2783\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2569 - val_loss: 0.2770\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2560 - val_loss: 0.2753\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2544 - val_loss: 0.2738\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2537 - val_loss: 0.2729\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2533 - val_loss: 0.2721\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2519 - val_loss: 0.2709\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2510 - val_loss: 0.2694\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2505 - val_loss: 0.2680\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2495 - val_loss: 0.2671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2487 - val_loss: 0.2665\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2482 - val_loss: 0.2659\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2478 - val_loss: 0.2652\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2467 - val_loss: 0.2645\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2472 - val_loss: 0.2638\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2456 - val_loss: 0.2629\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2450 - val_loss: 0.2619\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2442 - val_loss: 0.2610\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2444 - val_loss: 0.2605\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2433 - val_loss: 0.2600\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2436 - val_loss: 0.2594\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2427 - val_loss: 0.2586\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2419 - val_loss: 0.2578\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2413 - val_loss: 0.2572\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2415 - val_loss: 0.2567\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2416 - val_loss: 0.2559\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2405 - val_loss: 0.2549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c8b1f8b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_shape = (data2.shape[1],)\n",
    "\n",
    "# Define the model architecture\n",
    "encoder = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "decoder = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(64,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(data2.shape[1], activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder model\n",
    "autoencoder = keras.Sequential([encoder, decoder])\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(data2, data2, epochs=100, batch_size=256, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc1f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder model\n",
    "encoded_data = encoder.predict(data2)\n",
    "np.savetxt('latent_rppa.csv', encoded_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77e805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
